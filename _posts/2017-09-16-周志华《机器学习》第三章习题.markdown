---
layout: post
title:  "周志华《机器学习》第三章习题"
date:   2017-08-15 23:30:00
headerImage: false
tag:
- Machine Learning
category: blog
author: whyeemcc
description: none
---

### 3.1 试分析在什么情形下式（3.2）中不必考虑偏置项 b。

式（3.2）：

$$f\left( x \right)= \omega^T x+ b$$

答：偏置项b是将线性组合后的特征向量再整体对结果进行一个偏移。当输出空间的定义在特征空间的所有基的系数都为0，其结果也应该为0时，偏置项b可以不考虑。例如房价预测的回归问题，假设我们设定的特征有面积、房间个数、楼层数，当它们的系数都为0时，房价必然是0，这时b就没有作用了。

但是像这样过零点的“理想”样本很难出现在测试集中，所以我们一般会添加这样一个跟属性无关的偏置项，来保证回归的准确性，或者分类问题中超平面的可调性。

### 3.2 试证明，对于参数 $\omega$，对数几率回归的目标函数（3.18）是非凸的，但其对数似然函数（3.27）是凸的。

式（3.18）：

$$y=\frac{1}{1+{e}^{-( {\omega}^{T}x+b)}}$$

式（3.27）：

$$\imath (\beta )=\sum_{i=1}^{m}(-y_{i}\beta^{T}\hat{x}_{i}+ln(1+e^{\beta^{T}\hat{x}_{i}}))$$

答： 

(1) : 对于这种多元函数的二次微分，可以求出其Hessian矩阵，判断它的正定性，即可得出它的凹凸性质。

式（3.18）的一阶偏导为 $\frac{dy}{d\omega }=(y-y^2)\frac{d(\omega^{T}x)}{d\omega}$，其中$\frac{d(\omega^{T}x)}{d\omega}$为一个对角矩阵。因此它的二阶偏导为：

$$\frac{d^2y}{d\omega^2 }=(1-2y)\frac{d(\omega^{T}x)}{d\omega} \frac{dy}{d\omega}=(1-2y)(y-y^2)\frac{d(\omega^{T}x)}{d\omega}\frac{d(\omega^{T}x)}{d\omega}$$

简化为$(1-2y)(y-y^2)x^Tx$，在这里$x^Tx$是数值大于或等于0的平方项的对角矩阵，所以为半定矩阵，由于在sigmoid函数中，值域$0<y<1$，当$y>0.5$时，此Hessian矩阵非正定，因此是非凸的。

(2) : 

