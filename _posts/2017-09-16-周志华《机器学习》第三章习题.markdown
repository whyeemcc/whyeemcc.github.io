---
layout: post
title:  "周志华《机器学习》第三章习题"
date:   2017-08-15 23:30:00
headerImage: false
tag:
- Machine Learning
category: blog
author: whyeemcc
description: none
---

准备入坑机器学习，把周志华的《机器学习》习题做一做，打好基础，从第三章（线性模型）开始。

### 3.1 试分析在什么情形下式（3.2）中不必考虑偏置项 b。

式（3.2）：

$$f\left( x \right)= \omega^T x+ b$$

答：偏置项b是将线性组合后的特征向量再整体对结果进行一个偏移。当输出空间的定义在特征空间的所有基的系数都为0，其结果也应该为0时，偏置项b可以不考虑。例如房价预测的回归问题，假设我们设定的特征有面积、房间个数、楼层数，当它们的系数都为0时，房价必然是0，这时b就没有作用了。

但是像这样过零点的“理想”样本很难出现在测试集中，所以我们一般会添加这样一个跟属性无关的偏置项，来保证回归的准确性，或者分类问题中超平面的可调性。

### 3.2 试证明，对于参数 $\omega$，对数几率回归的目标函数（3.18）是非凸的，但其对数似然函数（3.27）是凸的。

式（3.18）：

$$y=\frac{1}{1+{e}^{-( {\omega}^{T}x+b)}}$$

式（3.27）：

$$\imath (\beta )=\sum_{i=1}^{m}(-y_{i}\beta^{T}\hat{x}_{i}+ln(1+e^{\beta^{T}\hat{x}_{i}}))$$

答： 

(1) : 对于这种多元函数的二阶微分，可以求出其Hessian矩阵，判断它的正定性，即可得出它的凹凸性质。

式（3.18）的一阶偏导为 $\frac{\partial y}{\partial\omega }=(y-y^2)\frac{\partial(\omega^{T}x)}{\partial\omega}$，其中$\frac{\partial(\omega^{T}x)}{\partial\omega}$为一个对角矩阵。因此它的二阶偏导为：

$$\frac{\partial^2y}{\partial\omega\partial\omega^T}=(1-2y)\frac{\partial(\omega^{T}x)}{\partial\omega} \frac{\partial y}{\partial\omega}=(1-2y)(y-y^2)\frac{\partial(\omega^{T}x)}{\partial\omega}\frac{\partial(\omega^{T}x)}{\partial\omega}$$

简化为$(1-2y)(y-y^2)x^Tx$，在这里$x^Tx$是数值大于或等于0的平方项的对角矩阵，所以为半正定矩阵，由于在sigmoid函数中，值域$0<y<1$，当$y>0.5$时，此Hessian矩阵非正定，因此是非凸的。

(2) : 式（3.27）的一阶偏导为：

$$\frac{\partial \imath (\beta)}{\partial\beta}=\sum_{i=1}^{m}(-y_{i}+\frac{e^{\beta^T\hat{x}_{i}}}{1+e^{\beta^T\hat{x}_{i}}})\frac{\partial(\beta^T\hat{x}_{i})}{\partial\beta}$$

二阶偏导为：

$$\frac{\partial^2 \imath (\beta)}{\partial \beta\partial\beta^T}=\sum_{i=1}^{m}\frac{e^{\beta^T\hat{x}_{i}}(1+e^{\beta^T\hat{x}_{i}})-e^{\beta^T\hat{x}_{i}}e^{\beta^T\hat{x}_{i}}}{(1+e^{\beta^T\hat{x}_{i}})^2}\frac{\partial(\beta^T\hat{x}_{i})}{\partial\beta}\frac{\partial(\beta^T\hat{x}_{i})}{\partial\beta}$$

进一步：

$$\frac{\partial^2 \imath (\beta)}{\partial \beta\partial\beta^T}=\sum_{i=1}^{m}\hat{x}_{i}^T\hat{x}_{i}\frac{e^{\beta^T\hat{x}_{i}}}{1+e^{\beta^T\hat{x}_{i}}}\frac{1}{1+e^{\beta^T\hat{x}_{i}}}$$

最终可写成后验概率为 $p(y=1\mid x)$ 的形式，即：

$$\frac{\partial^2 \imath (\beta)}{\partial \beta\partial\beta^T}=\sum_{i=1}^{m}\hat{x}_{i}^T\hat{x}_{i}p_1(\hat{x}_{i};\beta)(1-p_1(\hat{x}_{i};\beta))$$

由于 $0<p(y=1\mid x)<1$，所以为正定矩阵，则该式（对数似然函数）是凸的。


### 3.3 编程实现对率回归，并给出西瓜数据集 3.0a 上的结果。

答：书中西瓜数据集 3.0a 为：
			
编号 | 密度 | 含糖率 | 好瓜
-----|------|--------|------
1	 |0.697	|0.460	 |是
2	 |0.774	|0.376	 |是
3	 |0.634	|0.264	 |是
4	 |0.608	|0.318	 |是
5	 |0.556	|0.215	 |是
6	 |0.403	|0.237	 |是
7	 |0.481	|0.149	 |是
8	 |0.437	|0.211	 |是
9	 |0.666	|0.091	 |否
10	 |0.243	|0.267	 |否
11	 |0.245	|0.057	 |否
12	 |0.343	|0.099	 |否
13	 |0.639	|0.161	 |否
14	 |0.657	|0.198	 |否
15	 |0.360	|0.370	 |否
16	 |0.593	|0.042	 |否
17	 |0.719	|0.103	 |否
