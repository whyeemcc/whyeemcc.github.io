---
layout: post
title:  "周志华《机器学习》第三章习题"
date:   2017-08-15 23:30:00
headerImage: false
tag:
- Machine Learning
category: blog
author: whyeemcc
description: none
---

准备入坑机器学习，把周志华的《机器学习》习题做一做，打好基础，从第三章（线性模型）开始。

### 3.1 试分析在什么情形下式（3.2）中不必考虑偏置项 b。

式（3.2）：

$$f\left( x \right)= \omega^T x+ b$$

答：偏置项b是将线性组合后的特征向量再整体对结果进行一个偏移。当输出空间的定义在特征空间的所有基的系数都为0，其结果也应该为0时，偏置项b可以不考虑。例如房价预测的回归问题，假设我们设定的特征有面积、房间个数、楼层数，当它们的系数都为0时，房价必然是0，这时b就没有作用了。

但是像这样过零点的“理想”样本很难出现在测试集中，所以我们一般会添加这样一个跟属性无关的偏置项，来保证回归的准确性，或者分类问题中超平面的可调性。

### 3.2 试证明，对于参数 $\omega$，对数几率回归的目标函数（3.18）是非凸的，但其对数似然函数（3.27）是凸的。

式（3.18）：

$$y=\frac{1}{1+{e}^{-( {\omega}^{T}x+b)}}$$

式（3.27）：

$$\imath (\beta )=\sum_{i=1}^{m}(-y_{i}\beta^{T}\hat{x}_{i}+ln(1+e^{\beta^{T}\hat{x}_{i}}))$$

答： 

(1) : 对于这种多元函数的二阶微分，可以求出其Hessian矩阵，判断它的正定性，即可得出它的凹凸性质。

式（3.18）的一阶偏导为 $\frac{\partial y}{\partial\omega }=(y-y^2)\frac{\partial(\omega^{T}x)}{\partial\omega}$，其中$\frac{\partial(\omega^{T}x)}{\partial\omega}$为一个对角矩阵。因此它的二阶偏导为：

$$\frac{\partial^2y}{\partial\omega\partial\omega^T}=(1-2y)\frac{\partial(\omega^{T}x)}{\partial\omega} \frac{\partial y}{\partial\omega}=(1-2y)(y-y^2)\frac{\partial(\omega^{T}x)}{\partial\omega}\frac{\partial(\omega^{T}x)}{\partial\omega}$$

简化为$(1-2y)(y-y^2)x^Tx$，在这里$x^Tx$是数值大于或等于0的平方项的对角矩阵，所以为半正定矩阵，由于在sigmoid函数中，值域$0<y<1$，当$y>0.5$时，此Hessian矩阵非正定，因此是非凸的。

(2) : 式（3.27）的一阶偏导为：

$$\frac{\partial \imath (\beta)}{\partial\beta}=\sum_{i=1}^{m}(-y_{i}+\frac{e^{\beta^T\hat{x}_{i}}}{1+e^{\beta^T\hat{x}_{i}}})\frac{\partial(\beta^T\hat{x}_{i})}{\partial\beta}$$

二阶偏导为：

$$\frac{\partial^2 \imath (\beta)}{\partial \beta\partial\beta^T}=\sum_{i=1}^{m}\frac{e^{\beta^T\hat{x}_{i}}(1+e^{\beta^T\hat{x}_{i}})-e^{\beta^T\hat{x}_{i}}e^{\beta^T\hat{x}_{i}}}{(1+e^{\beta^T\hat{x}_{i}})^2}\frac{\partial(\beta^T\hat{x}_{i})}{\partial\beta}\frac{\partial(\beta^T\hat{x}_{i})}{\partial\beta}$$

进一步：

$$\frac{\partial^2 \imath (\beta)}{\partial \beta\partial\beta^T}=\sum_{i=1}^{m}\hat{x}_{i}^T\hat{x}_{i}\frac{e^{\beta^T\hat{x}_{i}}}{1+e^{\beta^T\hat{x}_{i}}}\frac{1}{1+e^{\beta^T\hat{x}_{i}}}$$

最终可写成后验概率为 $p(y=1\mid x)$ 的形式，即：

$$\frac{\partial^2 \imath (\beta)}{\partial \beta\partial\beta^T}=\sum_{i=1}^{m}\hat{x}_{i}^T\hat{x}_{i}p_1(\hat{x}_{i};\beta)(1-p_1(\hat{x}_{i};\beta))$$

由于 $0<p(y=1\mid x)<1$，所以为正定矩阵，则该式（对数似然函数）是凸的。


### 3.3 编程实现对率回归，并给出西瓜数据集 3.0a 上的结果。

答：书中西瓜数据集 3.0a 为：

编号 | 密度     | 含糖率 | 好瓜 
---  |---       |---     |---
1	 | 0.697	|0.460	 |是 
2	 | 0.774	|0.376	 |是 
3	 | 0.634	|0.264	 |是 
4	 | 0.608	|0.318	 |是 
5	 | 0.556	|0.215	 |是 
6	 | 0.403	|0.237	 |是 
7	 | 0.481	|0.149	 |是 
8	 | 0.437	|0.211	 |是 
9	 | 0.666	|0.091	 |否 
10	 | 0.243	|0.267	 |否 
11	 | 0.245	|0.057	 |否 
12	 | 0.343	|0.099	 |否 
13	 | 0.639	|0.161	 |否
14	 | 0.657	|0.198	 |否 
15	 | 0.360	|0.370	 |否 
16	 | 0.593	|0.042	 |否 
17	 | 0.719	|0.103	 |否 


这里的特征只有两个，即密度和含糖率，事先需要将样本标记，可以把好瓜标记为1，差瓜标记为0，然后保存到数据文件内，供代码读取。

```python
import sys,os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

path = sys.path[0]
csv_name = 'watermelon sample.csv'
data = pd.read_csv(path + '/' + csv_name, header=0, encoding='utf-8')

# sample count
m = len(data)

X = np.array([data['density'],
              data['sugar']]
            )
Y = np.array(data['result'])

# sigmoid function
def sigmoid(z):
    return 1/(1 + np.exp(-z))

# loss function
def loss(a,y):
    return -(y*np.log(a)+(1-y)*np.log(1-a))
    
# initialize the w and b
w = np.zeros((2,1))
b = 0

# learning rate
alpha = 2

loop = 1000
Loss_list = []
for i in range(loop):
    Z = np.dot(w.T,X) + b
    A = sigmoid(Z)
    # loss value
    L = 1/m*np.sum(loss(A,Y))
    Loss_list.append(L)
    # update parameters
    dZ = A - Y
    dw = 1/m*np.dot(X,dZ.T)
    db = 1/m*np.sum(dZ)
    w = w - alpha*dw
    b = b - alpha*db

red = data[data.result == 1]
blue = data[data.result == 0]    
# plot Loss    
plt.figure()  
ax1 = plt.subplot(211)        
ax1.plot(Loss_list)
ax1.set_title('Loss function')
# plot result
ax2 = plt.subplot(212)
x = np.arange(0.1,0.9,0.01)
y = -(w[0]*x+b)/w[1]
plt.plot(x,y,c='orange')
plt.scatter(red['density'],red['sugar'],c='red',label='good')
plt.scatter(blue['density'],blue['sugar'],marker='x',c='blue',label='bad')
plt.xlabel('density')     
plt.ylabel('sugar')     
plt.xlim(0.1,0.9)
plt.ylim(0,0.5)
plt.legend()
plt.show()
```